This is step by step to ensure all running succesfully.

1. Activate the virtual environment (Windows) by using this command:

.\venv\Scripts\activate

2. Then run this command to install all requirements:

pip install -r requirements.txt

3. Then, you need to download ollama to run LLM Model locally at https://ollama.com/

4. After done download it, install it.

5. Then open Command Prompt (CMD), then run this command:

ollama pull llama3

6. After finish, run this command to run it:

ollama run llama3

7. After done it, now you had llama3 run on your local machine.

8. After done it, now you can back to the folder before then run this command:

streamlit run app.py